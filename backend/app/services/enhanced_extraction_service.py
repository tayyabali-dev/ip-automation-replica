"""
Enhanced extraction service implementing the two-step extraction process.
Step 1: Evidence Gathering (The Scratchpad)
Step 2: JSON Generation with validation
"""

import json
import logging
import asyncio
from typing import Dict, List, Optional, Any, Union
from datetime import datetime

from app.models.enhanced_extraction import (
    DocumentEvidence, InventorEvidence, ApplicantEvidence, EvidenceItem,
    SourceLocation, ExtractionMethod, ConfidenceLevel, DataCompleteness,
    EnhancedExtractionResult, EnhancedInventor, EnhancedApplicant,
    QualityMetrics, ExtractionMetadata, ValidationResult,
    EvidenceGatheringError, DataProcessingError
)
from app.services.llm import LLMService, clean_fragmented_text
from app.core.config import settings

logger = logging.getLogger(__name__)

class EnhancedExtractionService:
    """
    Enhanced extraction service implementing systematic two-step extraction
    """
    
    def __init__(self, llm_service: LLMService = None):
        self.llm_service = llm_service or LLMService()
        self.evidence_gathering_prompts = EvidenceGatheringPrompts()
        self.json_generation_prompts = JSONGenerationPrompts()
        
    async def extract_with_two_step_process(
        self,
        file_path: str,
        file_content: Optional[bytes] = None,
        document_type: str = "unknown",
        progress_callback: Optional[callable] = None
    ) -> EnhancedExtractionResult:
        """
        Main entry point for two-step extraction process
        """
        start_time = datetime.utcnow()
        
        try:
            if progress_callback:
                await progress_callback(10, "Starting evidence gathering phase...")
            
            # Step 1: Evidence Gathering
            document_evidence = await self._gather_evidence_systematic(
                file_path, file_content, document_type, progress_callback
            )
            
            if progress_callback:
                await progress_callback(60, "Generating structured data from evidence...")
            
            # Step 2: JSON Generation from Evidence
            extraction_result = await self._generate_json_from_evidence(
                document_evidence, progress_callback
            )
            
            if progress_callback:
                await progress_callback(90, "Validating and finalizing results...")
            
            # Step 3: Validation and Quality Assessment
            validated_result = await self._validate_and_enhance_result(
                extraction_result, document_evidence
            )
            
            # Add processing metadata
            processing_time = (datetime.utcnow() - start_time).total_seconds()
            validated_result.extraction_metadata.processing_time = processing_time
            
            if progress_callback:
                await progress_callback(100, "Extraction completed successfully")
            
            return validated_result
            
        except Exception as e:
            logger.error(f"Two-step extraction failed: {e}", exc_info=True)
            raise DataProcessingError(
                f"Enhanced extraction failed: {str(e)}",
                error_code="EXTRACTION_FAILED",
                context={"file_path": file_path, "document_type": document_type}
            )
    
    async def _gather_evidence_systematic(
        self,
        file_path: str,
        file_content: Optional[bytes],
        document_type: str,
        progress_callback: Optional[callable] = None
    ) -> DocumentEvidence:
        """
        Step 1: Systematic evidence gathering from document
        """
        try:
            # Determine extraction method based on document analysis
            extraction_method = await self._determine_extraction_method(
                file_path, file_content
            )
            
            # Get appropriate evidence gathering prompt
            evidence_prompt = self.evidence_gathering_prompts.get_evidence_prompt(
                extraction_method, document_type
            )
            
            # Upload file for analysis if needed
            file_obj = None
            if extraction_method in [ExtractionMethod.VISION_ANALYSIS, ExtractionMethod.XFA_FORM]:
                upload_source = file_content if file_content else file_path
                file_obj = await self.llm_service.upload_file(upload_source)
            
            # Extract text content for text-based methods
            text_content = None
            if extraction_method in [ExtractionMethod.TEXT_EXTRACTION, ExtractionMethod.FORM_FIELDS]:
                text_content = await self.llm_service._extract_text_locally(file_path, file_content)
            
            # Generate evidence using LLM
            evidence_response = await self._generate_evidence_with_llm(
                evidence_prompt, file_obj, text_content
            )
            
            # Parse evidence response into structured format
            document_evidence = await self._parse_evidence_response(
                evidence_response, extraction_method
            )
            
            return document_evidence
            
        except Exception as e:
            logger.error(f"Evidence gathering failed: {e}", exc_info=True)
            raise EvidenceGatheringError(
                f"Failed to gather evidence: {str(e)}",
                error_code="EVIDENCE_GATHERING_FAILED",
                context={"file_path": file_path, "extraction_method": extraction_method}
            )
    
    async def _determine_extraction_method(
        self,
        file_path: str,
        file_content: Optional[bytes]
    ) -> ExtractionMethod:
        """
        Determine the best extraction method for the document
        """
        try:
            # Check for XFA forms first
            xfa_data = await self.llm_service._extract_xfa_data(file_path, file_content)
            if xfa_data:
                return ExtractionMethod.XFA_FORM
            
            # Check text extraction quality
            text_content = await self.llm_service._extract_text_locally(file_path, file_content)
            if text_content and len(text_content.strip()) > 500:
                # Check for form fields
                if "--- FORM FIELD DATA ---" in text_content:
                    return ExtractionMethod.FORM_FIELDS
                else:
                    return ExtractionMethod.TEXT_EXTRACTION
            
            # Fallback to vision analysis
            return ExtractionMethod.VISION_ANALYSIS
            
        except Exception as e:
            logger.warning(f"Could not determine extraction method: {e}")
            return ExtractionMethod.VISION_ANALYSIS
    
    async def _generate_evidence_with_llm(
        self,
        prompt: str,
        file_obj: Any = None,
        text_content: str = None
    ) -> Dict[str, Any]:
        """
        Generate evidence using LLM with appropriate input
        """
        try:
            if file_obj:
                # Multimodal analysis with file
                response = await self.llm_service.generate_structured_content(
                    prompt=prompt,
                    file_obj=file_obj,
                    retries=3
                )
            elif text_content:
                # Text-only analysis
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # APPLY FRAGMENTED TEXT CLEANING - Fix for word-per-line PDFs
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                original_length = len(text_content)
                cleaned_text_content = clean_fragmented_text(text_content)
                cleaned_length = len(cleaned_text_content)
                
                if original_length != cleaned_length:
                    logger.info(
                        f"Enhanced extraction text cleaned: {original_length} -> {cleaned_length} chars "
                        f"(removed {original_length - cleaned_length} fragmentation artifacts)"
                    )
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                
                full_prompt = f"{prompt}\n\n## DOCUMENT TEXT CONTENT:\n{cleaned_text_content[:50000]}"
                response = await self.llm_service.generate_structured_content(
                    prompt=full_prompt,
                    retries=3
                )
            else:
                raise ValueError("Either file_obj or text_content must be provided")
            
            return response
            
        except Exception as e:
            logger.error(f"LLM evidence generation failed: {e}")
            raise
    
    async def _parse_evidence_response(
        self,
        evidence_response: Union[Dict[str, Any], List[Any]],
        extraction_method: ExtractionMethod
    ) -> DocumentEvidence:
        """
        Parse LLM evidence response into structured DocumentEvidence
        """
        try:
            # CRITICAL FIX: Handle case where LLM returns a list instead of dict
            if isinstance(evidence_response, list):
                logger.warning("ðŸ” LLM returned list instead of dict, taking first element")
                if evidence_response and isinstance(evidence_response[0], dict):
                    evidence_response = evidence_response[0]
                else:
                    logger.error("âŒ LLM returned invalid list format")
                    evidence_response = {}
            
            # Initialize document evidence
            document_evidence = DocumentEvidence(
                document_pages=evidence_response.get("document_pages", 1),
                extraction_timestamp=datetime.utcnow()
            )
            
            # Parse title evidence - FIXED: Handle array, object, and evidence formats
            title_data = None
            if "title_evidence" in evidence_response:
                title_data = evidence_response["title_evidence"]
            elif "invention_title" in evidence_response:
                # Handle direct format from LLM
                title_info = evidence_response["invention_title"]
                if isinstance(title_info, list) and len(title_info) > 0:
                    # Handle array format - take first element
                    first_title = title_info[0]
                    if isinstance(first_title, dict) and first_title.get("text"):
                        title_data = {
                            "raw_text": first_title["text"],
                            "page": first_title.get("source_page", 1),
                            "section": first_title.get("source_section"),
                            "confidence": first_title.get("confidence", "medium")
                        }
                        logger.info(f"TITLE FOUND (array): {first_title['text']}")
                elif isinstance(title_info, dict):
                    # Handle both "text" and "raw_text" fields
                    text_value = title_info.get("text") or title_info.get("raw_text")
                    if text_value:
                        title_data = {
                            "raw_text": text_value,
                            "page": title_info.get("source_page", 1),
                            "section": title_info.get("source_section"),
                            "confidence": title_info.get("confidence", "medium")
                        }
                        logger.info(f"TITLE FOUND (dict): {text_value}")
            
            if title_data and title_data.get("raw_text"):
                document_evidence.title_evidence = EvidenceItem(
                    field_name="title",
                    raw_text=title_data["raw_text"],
                    source_location=SourceLocation(
                        page=title_data.get("page", 1),
                        section=title_data.get("section"),
                        raw_text=title_data["raw_text"],
                        extraction_method=extraction_method
                    ),
                    confidence=ConfidenceLevel(title_data.get("confidence", "medium"))
                )
            
            # Parse inventor evidence - FIXED: Handle both evidence format and direct format
            inventors_data = evidence_response.get("inventors_evidence", [])
            if not inventors_data:
                # Fallback: Check if LLM returned direct "inventors" format
                inventors_data = evidence_response.get("inventors", [])
                logger.info(f"EVIDENCE PARSING: Found {len(inventors_data)} inventors in direct format")
            else:
                logger.info(f"EVIDENCE PARSING: Found {len(inventors_data)} inventors in evidence format")
            
            for inv_data in inventors_data:
                inventor_evidence = self._parse_inventor_evidence(inv_data, extraction_method)
                document_evidence.inventor_evidence.append(inventor_evidence)
            
            # Parse applicant evidence - ENHANCED: Handle multiple applicant formats and sources
            applicants_data = evidence_response.get("applicants_evidence", [])
            if not applicants_data:
                # Fallback: Check multiple possible field names for applicants
                for field_name in ["applicants", "companies", "assignees", "organizations", "entities"]:
                    if field_name in evidence_response:
                        applicants_data = evidence_response[field_name]
                        logger.info(f"EVIDENCE PARSING: Found {len(applicants_data)} applicants in '{field_name}' format")
                        break
            else:
                logger.info(f"EVIDENCE PARSING: Found {len(applicants_data)} applicants in evidence format")
            
            # Parse primary applicant evidence
            for app_data in applicants_data:
                applicant_evidence = self._parse_applicant_evidence(app_data, extraction_method)
                document_evidence.applicant_evidence.append(applicant_evidence)
            
            # ENHANCED: Extract secondary applicant evidence from other sections
            secondary_applicants = self._extract_secondary_applicant_evidence(
                evidence_response, extraction_method
            )
            document_evidence.applicant_evidence.extend(secondary_applicants)
            
            # ENHANCED: Deduplicate applicant candidates
            document_evidence.applicant_evidence = self._deduplicate_applicant_candidates(
                document_evidence.applicant_evidence
            )
            
            # Parse other evidence fields
            self._parse_additional_evidence(evidence_response, document_evidence, extraction_method)
            
            # DIAGNOSTIC LOGGING: Track what evidence was actually parsed
            logger.info(f"PARSED EVIDENCE SUMMARY:")
            logger.info(f"  - Title: {'FOUND' if document_evidence.title_evidence else 'MISSING'}")
            logger.info(f"  - App Number: {'FOUND' if document_evidence.application_number_evidence else 'MISSING'}")
            logger.info(f"  - Entity Status: {'FOUND' if document_evidence.entity_status_evidence else 'MISSING'}")
            logger.info(f"  - Inventors: {len(document_evidence.inventor_evidence)}")
            logger.info(f"  - Applicants: {len(document_evidence.applicant_evidence)}")
            
            return document_evidence
            
        except Exception as e:
            logger.error(f"Evidence parsing failed: {e}")
            raise DataProcessingError(f"Failed to parse evidence: {str(e)}")
    
    def _parse_inventor_evidence(
        self,
        inv_data: Dict[str, Any],
        extraction_method: ExtractionMethod
    ) -> InventorEvidence:
        """
        Parse individual inventor evidence
        """
        inventor_evidence = InventorEvidence(
            sequence_number=inv_data.get("sequence_number"),
            completeness=DataCompleteness(inv_data.get("completeness", "incomplete")),
            overall_confidence=ConfidenceLevel(inv_data.get("confidence", "medium"))
        )
        
        # Parse name evidence - FIXED: Handle actual LLM response structure
        # LLM returns "name" field instead of separate "given_name" and "family_name"
        if "name" in inv_data and inv_data["name"]:
            name_data = inv_data["name"]
            if isinstance(name_data, str):
                # Parse full name into given and family names
                name_parts = name_data.strip().split()
                if len(name_parts) >= 2:
                    given_name = name_parts[0]
                    family_name = " ".join(name_parts[1:])
                    
                    # Create given name evidence
                    inventor_evidence.given_name_evidence = EvidenceItem(
                        field_name="given_name",
                        raw_text=given_name,
                        source_location=SourceLocation(
                            page=inv_data.get("source", {}).get("page", 1),
                            section=inv_data.get("source", {}).get("section", "inventor_info"),
                            raw_text=given_name,
                            extraction_method=extraction_method
                        ),
                        confidence=ConfidenceLevel("medium")
                    )
                    
                    # Create family name evidence
                    inventor_evidence.family_name_evidence = EvidenceItem(
                        field_name="family_name",
                        raw_text=family_name,
                        source_location=SourceLocation(
                            page=inv_data.get("source", {}).get("page", 1),
                            section=inv_data.get("source", {}).get("section", "inventor_info"),
                            raw_text=family_name,
                            extraction_method=extraction_method
                        ),
                        confidence=ConfidenceLevel("medium")
                    )
                    
                    logger.info(f"INVENTOR NAME PARSED: Given='{given_name}', Family='{family_name}'")
                else:
                    # Single name - treat as given name
                    inventor_evidence.given_name_evidence = EvidenceItem(
                        field_name="given_name",
                        raw_text=name_data,
                        source_location=SourceLocation(
                            page=inv_data.get("source", {}).get("page", 1),
                            section=inv_data.get("source", {}).get("section", "inventor_info"),
                            raw_text=name_data,
                            extraction_method=extraction_method
                        ),
                        confidence=ConfidenceLevel("medium")
                    )
                    logger.info(f"INVENTOR SINGLE NAME: {name_data}")
        
        # Fallback: Check for separate given_name and family_name fields (legacy format)
        elif "given_name" in inv_data and inv_data["given_name"]:
            given_name_data = inv_data["given_name"]
            if isinstance(given_name_data, dict) and "raw_text" in given_name_data:
                # Evidence format
                raw_text = given_name_data["raw_text"]
                page = given_name_data.get("page", 1)
                section = given_name_data.get("section")
                confidence = given_name_data.get("confidence", "medium")
            else:
                # Direct format - extract text field if it's a dict, otherwise convert to string
                if isinstance(given_name_data, dict) and "text" in given_name_data:
                    raw_text = given_name_data["text"]
                    page = given_name_data.get("source_page", 1)
                    section = given_name_data.get("source_section", "inventor_info")
                    confidence = given_name_data.get("confidence", "medium")
                else:
                    raw_text = str(given_name_data)
                    page = 1
                    section = "inventor_info"
                    confidence = "medium"
                logger.info(f"INVENTOR GIVEN NAME (legacy): {raw_text}")
            
            inventor_evidence.given_name_evidence = EvidenceItem(
                field_name="given_name",
                raw_text=raw_text,
                source_location=SourceLocation(
                    page=page,
                    section=section,
                    raw_text=raw_text,
                    extraction_method=extraction_method
                ),
                confidence=ConfidenceLevel(confidence)
            )
            
            # Parse family name evidence for legacy format
            if "family_name" in inv_data and inv_data["family_name"]:
                family_name_data = inv_data["family_name"]
                if isinstance(family_name_data, dict) and "raw_text" in family_name_data:
                    # Evidence format
                    raw_text = family_name_data["raw_text"]
                    page = family_name_data.get("page", 1)
                    section = family_name_data.get("section")
                    confidence = family_name_data.get("confidence", "medium")
                else:
                    # Direct format - extract text field if it's a dict, otherwise convert to string
                    if isinstance(family_name_data, dict) and "text" in family_name_data:
                        raw_text = family_name_data["text"]
                        page = family_name_data.get("source_page", 1)
                        section = family_name_data.get("source_section", "inventor_info")
                        confidence = family_name_data.get("confidence", "medium")
                    else:
                        raw_text = str(family_name_data)
                        page = 1
                        section = "inventor_info"
                        confidence = "medium"
                    logger.info(f"INVENTOR FAMILY NAME (legacy): {raw_text}")
                
                inventor_evidence.family_name_evidence = EvidenceItem(
                    field_name="family_name",
                    raw_text=raw_text,
                    source_location=SourceLocation(
                        page=page,
                        section=section,
                        raw_text=raw_text,
                        extraction_method=extraction_method
                    ),
                    confidence=ConfidenceLevel(confidence)
                )
        
        # Parse address evidence - FIXED: Handle actual LLM response structure
        # LLM returns "address" field as string, not "address_evidence" array
        if "address" in inv_data and inv_data["address"]:
            address_text = inv_data["address"]
            if isinstance(address_text, str) and address_text.strip():
                inventor_evidence.address_evidence.append(EvidenceItem(
                    field_name="address",
                    raw_text=address_text,
                    source_location=SourceLocation(
                        page=inv_data.get("source", {}).get("page", 1),
                        section=inv_data.get("source", {}).get("section", "inventor_info"),
                        raw_text=address_text,
                        extraction_method=extraction_method
                    ),
                    confidence=ConfidenceLevel("medium")
                ))
                logger.info(f"INVENTOR ADDRESS PARSED: {address_text}")
        
        # Fallback: Check for legacy address_evidence format
        address_data = inv_data.get("address_evidence", [])
        for addr_item in address_data:
            if addr_item and addr_item.get("raw_text"):
                inventor_evidence.address_evidence.append(EvidenceItem(
                    field_name=addr_item.get("field_name", "address"),
                    raw_text=addr_item["raw_text"],
                    source_location=SourceLocation(
                        page=addr_item.get("page", 1),
                        section=addr_item.get("section"),
                        raw_text=addr_item["raw_text"],
                        extraction_method=extraction_method
                    ),
                    confidence=ConfidenceLevel(addr_item.get("confidence", "medium"))
                ))
        
        return inventor_evidence
    
    def _parse_applicant_evidence(
        self,
        app_data: Dict[str, Any],
        extraction_method: ExtractionMethod
    ) -> ApplicantEvidence:
        """
        Parse individual applicant evidence
        """
        applicant_evidence = ApplicantEvidence(
            completeness=DataCompleteness(app_data.get("completeness", "incomplete")),
            overall_confidence=ConfidenceLevel(app_data.get("confidence", "medium"))
        )
        
        # Parse organization name evidence - FIXED: Handle both nested and direct formats
        if "organization_name" in app_data and app_data["organization_name"]:
            org_name_data = app_data["organization_name"]
            if isinstance(org_name_data, dict) and "raw_text" in org_name_data:
                # Evidence format
                raw_text = org_name_data["raw_text"]
                page = org_name_data.get("page", 1)
                section = org_name_data.get("section")
                confidence = org_name_data.get("confidence", "medium")
            elif isinstance(org_name_data, dict) and "text" in org_name_data:
                # Direct format with text field
                raw_text = org_name_data["text"]
                page = org_name_data.get("source_page", 1)
                section = org_name_data.get("source_section")
                confidence = org_name_data.get("confidence", "medium")
                logger.info(f"APPLICANT ORG NAME (direct): {raw_text}")
            else:
                # Direct string format
                raw_text = str(org_name_data)
                page = 1
                section = "applicant_info"
                confidence = "medium"
                logger.info(f"APPLICANT ORG NAME (string): {raw_text}")
            
            applicant_evidence.organization_name_evidence = EvidenceItem(
                field_name="organization_name",
                raw_text=raw_text,
                source_location=SourceLocation(
                    page=page,
                    section=section,
                    raw_text=raw_text,
                    extraction_method=extraction_method
                ),
                confidence=ConfidenceLevel(confidence)
            )
        
        # Parse address evidence - ENHANCED: Handle multiple address formats
        address_data = app_data.get("address_evidence", [])
        
        # Also check for direct address field (similar to inventor parsing)
        if "address" in app_data and app_data["address"]:
            address_text = app_data["address"]
            if isinstance(address_text, str) and address_text.strip():
                applicant_evidence.address_evidence.append(EvidenceItem(
                    field_name="address",
                    raw_text=address_text,
                    source_location=SourceLocation(
                        page=app_data.get("source", {}).get("page", 1),
                        section=app_data.get("source", {}).get("section", "applicant_info"),
                        raw_text=address_text,
                        extraction_method=extraction_method
                    ),
                    confidence=ConfidenceLevel("medium")
                ))
                logger.info(f"APPLICANT ADDRESS PARSED: {address_text}")
        
        # Parse structured address evidence
        for addr_item in address_data:
            if addr_item and addr_item.get("raw_text"):
                applicant_evidence.address_evidence.append(EvidenceItem(
                    field_name=addr_item.get("field_name", "address"),
                    raw_text=addr_item["raw_text"],
                    source_location=SourceLocation(
                        page=addr_item.get("page", 1),
                        section=addr_item.get("section"),
                        raw_text=addr_item["raw_text"],
                        extraction_method=extraction_method
                    ),
                    confidence=ConfidenceLevel(addr_item.get("confidence", "medium"))
                ))
        
        # Parse individual name evidence for individual applicants
        if "individual_given_name" in app_data or "individual_family_name" in app_data:
            if app_data.get("individual_given_name"):
                given_name_data = app_data["individual_given_name"]
                raw_text = str(given_name_data) if not isinstance(given_name_data, dict) else given_name_data.get("text", str(given_name_data))
                
                individual_name_evidence = EvidenceItem(
                    field_name="individual_given_name",
                    raw_text=raw_text,
                    source_location=SourceLocation(
                        page=app_data.get("source", {}).get("page", 1),
                        section=app_data.get("source", {}).get("section", "applicant_info"),
                        raw_text=raw_text,
                        extraction_method=extraction_method
                    ),
                    confidence=ConfidenceLevel("medium")
                )
                applicant_evidence.individual_name_evidence.append(individual_name_evidence)
                logger.info(f"INDIVIDUAL APPLICANT GIVEN NAME: {raw_text}")
            
            if app_data.get("individual_family_name"):
                family_name_data = app_data["individual_family_name"]
                raw_text = str(family_name_data) if not isinstance(family_name_data, dict) else family_name_data.get("text", str(family_name_data))
                
                individual_name_evidence = EvidenceItem(
                    field_name="individual_family_name",
                    raw_text=raw_text,
                    source_location=SourceLocation(
                        page=app_data.get("source", {}).get("page", 1),
                        section=app_data.get("source", {}).get("section", "applicant_info"),
                        raw_text=raw_text,
                        extraction_method=extraction_method
                    ),
                    confidence=ConfidenceLevel("medium")
                )
                applicant_evidence.individual_name_evidence.append(individual_name_evidence)
                logger.info(f"INDIVIDUAL APPLICANT FAMILY NAME: {raw_text}")
        
        # Parse contact evidence (customer numbers, emails, etc.)
        contact_fields = ["customer_number", "email_address", "phone_number"]
        for field in contact_fields:
            if field in app_data and app_data[field]:
                contact_data = app_data[field]
                raw_text = str(contact_data) if not isinstance(contact_data, dict) else contact_data.get("text", str(contact_data))
                
                if raw_text and raw_text.strip():
                    contact_evidence = EvidenceItem(
                        field_name=field,
                        raw_text=raw_text,
                        source_location=SourceLocation(
                            page=app_data.get("source", {}).get("page", 1),
                            section=app_data.get("source", {}).get("section", "applicant_info"),
                            raw_text=raw_text,
                            extraction_method=extraction_method
                        ),
                        confidence=ConfidenceLevel("medium")
                    )
                    applicant_evidence.contact_evidence.append(contact_evidence)
                    logger.info(f"APPLICANT {field.upper()}: {raw_text}")
        
        return applicant_evidence
    
    def _extract_secondary_applicant_evidence(
        self,
        evidence_response: Dict[str, Any],
        extraction_method: ExtractionMethod
    ) -> List[ApplicantEvidence]:
        """
        Extract applicant evidence from secondary sources (headers, correspondence, etc.)
        """
        secondary_applicants = []
        
        try:
            # Check for applicants in correspondence sections
            correspondence_data = evidence_response.get("correspondence_evidence", [])
            for corr_item in correspondence_data:
                if corr_item and corr_item.get("raw_text"):
                    # Look for company names in correspondence
                    raw_text = corr_item["raw_text"]
                    if self._contains_company_indicators(raw_text):
                        applicant_evidence = ApplicantEvidence(
                            completeness=DataCompleteness.PARTIAL_NAME,
                            overall_confidence=ConfidenceLevel.LOW
                        )
                        
                        # Extract potential company name
                        company_name = self._extract_company_name_from_text(raw_text)
                        if company_name:
                            applicant_evidence.organization_name_evidence = EvidenceItem(
                                field_name="organization_name",
                                raw_text=company_name,
                                source_location=SourceLocation(
                                    page=corr_item.get("page", 1),
                                    section="correspondence",
                                    raw_text=raw_text,
                                    extraction_method=extraction_method
                                ),
                                confidence=ConfidenceLevel.LOW
                            )
                            secondary_applicants.append(applicant_evidence)
            
            # Check for applicants in header/footer information
            for section_name in ["header_evidence", "footer_evidence", "letterhead_evidence"]:
                section_data = evidence_response.get(section_name, [])
                for item in section_data:
                    if item and item.get("raw_text"):
                        raw_text = item["raw_text"]
                        if self._contains_company_indicators(raw_text):
                            company_name = self._extract_company_name_from_text(raw_text)
                            if company_name:
                                applicant_evidence = ApplicantEvidence(
                                    completeness=DataCompleteness.PARTIAL_NAME,
                                    overall_confidence=ConfidenceLevel.LOW
                                )
                                applicant_evidence.organization_name_evidence = EvidenceItem(
                                    field_name="organization_name",
                                    raw_text=company_name,
                                    source_location=SourceLocation(
                                        page=item.get("page", 1),
                                        section=section_name.replace("_evidence", ""),
                                        raw_text=raw_text,
                                        extraction_method=extraction_method
                                    ),
                                    confidence=ConfidenceLevel.LOW
                                )
                                secondary_applicants.append(applicant_evidence)
            
            logger.info(f"SECONDARY APPLICANT SEARCH: Found {len(secondary_applicants)} additional applicant candidates")
            
        except Exception as e:
            logger.warning(f"Secondary applicant extraction failed: {e}")
        
        return secondary_applicants
    
    def _contains_company_indicators(self, text: str) -> bool:
        """
        Check if text contains company/organization indicators
        """
        company_suffixes = ["Inc.", "LLC", "Corp.", "Corporation", "Ltd.", "Co.", "Company", "LLP"]
        company_keywords = ["Corporation", "Company", "Industries", "Technologies", "Systems", "Solutions"]
        
        text_upper = text.upper()
        return any(suffix.upper() in text_upper for suffix in company_suffixes) or \
               any(keyword.upper() in text_upper for keyword in company_keywords)
    
    def _extract_company_name_from_text(self, text: str) -> Optional[str]:
        """
        Extract potential company name from text
        """
        import re
        
        # Look for patterns like "Company Name Inc." or "Company Name Corporation"
        company_pattern = r'([A-Z][a-zA-Z\s&]+(?:Inc\.|LLC|Corp\.|Corporation|Ltd\.|Co\.|Company|LLP))'
        matches = re.findall(company_pattern, text)
        
        if matches:
            # Return the longest match (most likely to be complete company name)
            return max(matches, key=len).strip()
        
        return None
    
    def _deduplicate_applicant_candidates(
        self,
        applicant_candidates: List[ApplicantEvidence]
    ) -> List[ApplicantEvidence]:
        """
        Deduplicate applicant candidates using similarity matching
        """
        if len(applicant_candidates) <= 1:
            return applicant_candidates
        
        unique_applicants = []
        
        for candidate in applicant_candidates:
            is_duplicate = False
            
            for existing in unique_applicants:
                if self._are_applicants_similar(candidate, existing):
                    # Merge information from duplicate into existing
                    self._merge_applicant_evidence(existing, candidate)
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_applicants.append(candidate)
        
        logger.info(f"DEDUPLICATION: Reduced {len(applicant_candidates)} candidates to {len(unique_applicants)} unique applicants")
        return unique_applicants
    
    def _are_applicants_similar(
        self,
        candidate1: ApplicantEvidence,
        candidate2: ApplicantEvidence
    ) -> bool:
        """
        Check if two applicant candidates are likely the same entity
        """
        # Compare organization names if both have them
        if (candidate1.organization_name_evidence and candidate2.organization_name_evidence):
            name1 = candidate1.organization_name_evidence.raw_text.lower().strip()
            name2 = candidate2.organization_name_evidence.raw_text.lower().strip()
            
            # Simple similarity check - can be enhanced with fuzzy matching
            if name1 == name2:
                return True
            
            # Check if one name is contained in the other (e.g., "TechCorp" vs "TechCorp Inc.")
            if name1 in name2 or name2 in name1:
                return True
        
        return False
    
    def _merge_applicant_evidence(
        self,
        primary: ApplicantEvidence,
        secondary: ApplicantEvidence
    ):
        """
        Merge evidence from secondary applicant into primary, keeping the best quality data
        """
        # Merge address evidence
        primary.address_evidence.extend(secondary.address_evidence)
        
        # Merge contact evidence
        primary.contact_evidence.extend(secondary.contact_evidence)
        
        # Update completeness if secondary has better completeness
        # Note: DataCompleteness enum values need to be compared properly
        completeness_order = {
            DataCompleteness.INCOMPLETE: 0,
            DataCompleteness.PARTIAL_ADDRESS: 1,
            DataCompleteness.PARTIAL_NAME: 2,
            DataCompleteness.NAME_ONLY: 3,
            DataCompleteness.ADDRESS_ONLY: 4,
            DataCompleteness.COMPLETE: 5
        }
        
        if completeness_order.get(secondary.completeness, 0) > completeness_order.get(primary.completeness, 0):
            primary.completeness = secondary.completeness
        
        # Update confidence if secondary has higher confidence
        confidence_order = {
            ConfidenceLevel.LOW: 0,
            ConfidenceLevel.MEDIUM: 1,
            ConfidenceLevel.HIGH: 2
        }
        
        if confidence_order.get(secondary.overall_confidence, 0) > confidence_order.get(primary.overall_confidence, 0):
            primary.overall_confidence = secondary.overall_confidence
        
        # If secondary has better organization name evidence, use it
        if (secondary.organization_name_evidence and
            (not primary.organization_name_evidence or
             confidence_order.get(secondary.organization_name_evidence.confidence, 0) >
             confidence_order.get(primary.organization_name_evidence.confidence, 0))):
            primary.organization_name_evidence = secondary.organization_name_evidence
    
    def _parse_additional_evidence(
        self,
        evidence_response: Dict[str, Any],
        document_evidence: DocumentEvidence,
        extraction_method: ExtractionMethod
    ):
        """
        Parse additional evidence fields (application number, dates, entity status, etc.)
        """
        # Application number evidence - FIXED: Handle array, object, and evidence formats
        app_num_data = None
        if "application_number_evidence" in evidence_response:
            app_num_data = evidence_response["application_number_evidence"]
        elif "application_number" in evidence_response:
            # Handle direct format from LLM
            app_info = evidence_response["application_number"]
            if isinstance(app_info, list) and len(app_info) > 0:
                # Handle array format - take first element
                first_app = app_info[0]
                if isinstance(first_app, dict) and first_app.get("text"):
                    app_num_data = {
                        "raw_text": first_app["text"],
                        "page": first_app.get("source_page", 1),
                        "section": first_app.get("source_section"),
                        "confidence": first_app.get("confidence", "medium")
                    }
                    logger.info(f"APPLICATION NUMBER FOUND (array): {first_app['text']}")
            elif isinstance(app_info, dict):
                # Handle both "text" and "raw_text" fields
                text_value = app_info.get("text") or app_info.get("raw_text")
                if text_value:
                    app_num_data = {
                        "raw_text": text_value,
                        "page": app_info.get("source_page", 1),
                        "section": app_info.get("source_section"),
                        "confidence": app_info.get("confidence", "medium")
                    }
                    logger.info(f"APPLICATION NUMBER FOUND (dict): {text_value}")
        
        if app_num_data and app_num_data.get("raw_text"):
            document_evidence.application_number_evidence = EvidenceItem(
                field_name="application_number",
                raw_text=app_num_data["raw_text"],
                source_location=SourceLocation(
                    page=app_num_data.get("page", 1),
                    section=app_num_data.get("section"),
                    raw_text=app_num_data["raw_text"],
                    extraction_method=extraction_method
                ),
                confidence=ConfidenceLevel(app_num_data.get("confidence", "medium"))
            )
        
        # Filing date evidence
        if "filing_date_evidence" in evidence_response:
            date_data = evidence_response["filing_date_evidence"]
            if date_data and date_data.get("raw_text"):
                document_evidence.filing_date_evidence = EvidenceItem(
                    field_name="filing_date",
                    raw_text=date_data["raw_text"],
                    source_location=SourceLocation(
                        page=date_data.get("page", 1),
                        section=date_data.get("section"),
                        raw_text=date_data["raw_text"],
                        extraction_method=extraction_method
                    ),
                    confidence=ConfidenceLevel(date_data.get("confidence", "medium"))
                )
        
        # Entity status evidence - COMPREHENSIVE FIX: Handle all possible formats
        entity_data = None
        
        # Check all possible field names for entity status
        entity_fields = ["entity_status_evidence", "entity_status", "entity_status_information"]
        
        for field_name in entity_fields:
            if field_name in evidence_response:
                entity_info = evidence_response[field_name]
                logger.info(f"ENTITY STATUS RAW DATA ({field_name}): {entity_info}")
                
                if entity_info and entity_info != "null" and entity_info is not None:
                    if isinstance(entity_info, list) and len(entity_info) > 0:
                        # Handle array format - take first element
                        first_entity = entity_info[0]
                        if isinstance(first_entity, dict):
                            text_value = first_entity.get("text") or first_entity.get("raw_text") or first_entity.get("value")
                            if text_value:
                                entity_data = {
                                    "raw_text": text_value,
                                    "page": first_entity.get("source_page", 1),
                                    "section": first_entity.get("source_section", "entity_status"),
                                    "confidence": first_entity.get("confidence", "medium")
                                }
                                logger.info(f"ENTITY STATUS FOUND (array): {text_value}")
                                break
                        elif isinstance(first_entity, str):
                            entity_data = {
                                "raw_text": first_entity,
                                "page": 1,
                                "section": "entity_status",
                                "confidence": "medium"
                            }
                            logger.info(f"ENTITY STATUS FOUND (array string): {first_entity}")
                            break
                    elif isinstance(entity_info, dict):
                        text_value = entity_info.get("text") or entity_info.get("raw_text") or entity_info.get("value")
                        if text_value:
                            entity_data = {
                                "raw_text": text_value,
                                "page": entity_info.get("source_page", 1),
                                "section": entity_info.get("source_section", "entity_status"),
                                "confidence": entity_info.get("confidence", "medium")
                            }
                            logger.info(f"ENTITY STATUS FOUND (dict): {text_value}")
                            break
                    elif isinstance(entity_info, str):
                        entity_data = {
                            "raw_text": entity_info,
                            "page": 1,
                            "section": "entity_status",
                            "confidence": "medium"
                        }
                        logger.info(f"ENTITY STATUS FOUND (string): {entity_info}")
                        break
        
        if entity_data and entity_data.get("raw_text"):
            document_evidence.entity_status_evidence = EvidenceItem(
                field_name="entity_status",
                raw_text=entity_data["raw_text"],
                source_location=SourceLocation(
                    page=entity_data.get("page", 1),
                    section=entity_data.get("section"),
                    raw_text=entity_data["raw_text"],
                    extraction_method=extraction_method
                ),
                confidence=ConfidenceLevel(entity_data.get("confidence", "medium"))
            )
            logger.info(f"ENTITY STATUS EVIDENCE PARSED: {entity_data['raw_text']}")
        else:
            logger.warning("No entity_status_evidence in LLM response")
        
        # Correspondence evidence
        correspondence_data = evidence_response.get("correspondence_evidence", [])
        for corr_item in correspondence_data:
            if corr_item and corr_item.get("raw_text"):
                document_evidence.correspondence_evidence.append(EvidenceItem(
                    field_name=corr_item.get("field_name", "correspondence"),
                    raw_text=corr_item["raw_text"],
                    source_location=SourceLocation(
                        page=corr_item.get("page", 1),
                        section=corr_item.get("section"),
                        raw_text=corr_item["raw_text"],
                        extraction_method=extraction_method
                    ),
                    confidence=ConfidenceLevel(corr_item.get("confidence", "medium"))
                ))
    
    async def _generate_json_from_evidence(
        self,
        document_evidence: DocumentEvidence,
        progress_callback: Optional[callable] = None
    ) -> EnhancedExtractionResult:
        """
        Step 2: Generate structured JSON from gathered evidence
        """
        try:
            # Create JSON generation prompt with evidence
            json_prompt = self.json_generation_prompts.create_json_generation_prompt(
                document_evidence
            )
            
            # Generate structured data using LLM
            json_response = await self.llm_service.generate_structured_content(
                prompt=json_prompt,
                retries=3
            )
            
            # Convert to EnhancedExtractionResult
            extraction_result = await self._convert_to_extraction_result(
                json_response, document_evidence
            )
            
            return extraction_result
            
        except Exception as e:
            logger.error(f"JSON generation from evidence failed: {e}")
            raise DataProcessingError(f"Failed to generate JSON from evidence: {str(e)}")
    
    async def _convert_to_extraction_result(
        self,
        json_response: Dict[str, Any],
        document_evidence: DocumentEvidence
    ) -> EnhancedExtractionResult:
        """
        Convert JSON response to EnhancedExtractionResult
        """
        # Initialize extraction result
        result = EnhancedExtractionResult(
            title=json_response.get("title"),
            application_number=json_response.get("application_number"),
            filing_date=json_response.get("filing_date"),
            entity_status=json_response.get("entity_status"),
            attorney_docket_number=json_response.get("attorney_docket_number"),
            total_drawing_sheets=json_response.get("total_drawing_sheets"),
            customer_number=json_response.get("customer_number"),
            correspondence_email=json_response.get("correspondence_email"),
            document_evidence=document_evidence,
            extraction_metadata=ExtractionMetadata(
                extraction_method=ExtractionMethod.TEXT_EXTRACTION,  # Will be updated
                document_type="patent_application",
                processing_time=0.0  # Will be updated
            ),
            quality_metrics=QualityMetrics(
                completeness_score=0.0,  # Will be calculated
                accuracy_score=0.0,      # Will be calculated
                confidence_score=0.0,    # Will be calculated
                consistency_score=0.0,   # Will be calculated
                overall_quality_score=0.0, # Will be calculated
                required_fields_populated=0,
                total_required_fields=0,
                optional_fields_populated=0,
                total_optional_fields=0,
                validation_errors=0,
                validation_warnings=0
            )
        )
        
        # Convert inventors
        inventors_data = json_response.get("inventors", [])
        for inv_data in inventors_data:
            inventor = EnhancedInventor(
                given_name=inv_data.get("given_name"),
                middle_name=inv_data.get("middle_name"),
                family_name=inv_data.get("family_name"),
                full_name=inv_data.get("full_name"),
                street_address=inv_data.get("street_address"),
                city=inv_data.get("city"),
                state=inv_data.get("state"),
                postal_code=inv_data.get("postal_code"),
                country=inv_data.get("country"),
                citizenship=inv_data.get("citizenship"),
                completeness=DataCompleteness(inv_data.get("completeness", "incomplete")),
                confidence_score=float(inv_data.get("confidence_score") or 0.5)
            )
            result.inventors.append(inventor)
        
        # Convert applicants
        applicants_data = json_response.get("applicants", [])
        for app_data in applicants_data:
            applicant = EnhancedApplicant(
                is_assignee=bool(app_data.get("is_assignee", False)),
                organization_name=app_data.get("organization_name"),
                individual_given_name=app_data.get("individual_given_name"),
                individual_family_name=app_data.get("individual_family_name"),
                street_address=app_data.get("street_address"),
                city=app_data.get("city"),
                state=app_data.get("state"),
                postal_code=app_data.get("postal_code"),
                country=app_data.get("country"),
                customer_number=app_data.get("customer_number"),
                email_address=app_data.get("email_address"),
                relationship_to_inventors=app_data.get("relationship_to_inventors", "separate_entity"),
                completeness=DataCompleteness(app_data.get("completeness", "incomplete")),
                confidence_score=float(app_data.get("confidence_score") or 0.5)
            )
            result.applicants.append(applicant)
        
        return result
    
    async def _validate_and_enhance_result(
        self,
        extraction_result: EnhancedExtractionResult,
        document_evidence: DocumentEvidence
    ) -> EnhancedExtractionResult:
        """
        Step 3: Validate and enhance the extraction result
        """
        # This will be implemented with the validation framework
        # For now, return the result as-is
        return extraction_result


class EvidenceGatheringPrompts:
    """
    Prompts for evidence gathering phase
    """
    
    def get_evidence_prompt(
        self,
        extraction_method: ExtractionMethod,
        document_type: str
    ) -> str:
        """
        Get appropriate evidence gathering prompt based on extraction method
        """
        base_prompt = self._get_base_evidence_prompt()
        
        if extraction_method == ExtractionMethod.XFA_FORM:
            return base_prompt + self._get_xfa_specific_instructions()
        elif extraction_method == ExtractionMethod.FORM_FIELDS:
            return base_prompt + self._get_form_fields_instructions()
        elif extraction_method == ExtractionMethod.VISION_ANALYSIS:
            return base_prompt + self._get_vision_analysis_instructions()
        else:
            return base_prompt + self._get_text_extraction_instructions()
    
    def _get_base_evidence_prompt(self) -> str:
        """
        Enhanced base evidence gathering prompt with comprehensive multi-applicant detection
        """
        return """
You are a highly accurate Data Extraction Specialist analyzing a USPTO patent document. Your critical mission is to find ALL applicants/companies, not just the first one.

**CORE RULES & LOGIC:**
- An 'applicant' can be a company/organization OR an individual person
- Multiple applicants are common in patent documents
- Applicants may appear in different sections with varying levels of detail
- Company information is distinct from inventor information
- The same applicant may be mentioned in multiple sections

**CRITICAL INSTRUCTIONS:**
1. **SCAN THE ENTIRE DOCUMENT** - Check ALL pages systematically
2. **QUOTE RAW TEXT** - Extract exact text, never paraphrase or interpret
3. **NO HALLUCINATION** - Only extract what is visually present in the document
4. **DOCUMENT SOURCES** - Note page numbers and sections for all findings
5. **BE EXHAUSTIVE** - Better to over-extract than miss critical data
6. **FIND ALL APPLICANTS** - Do NOT stop after finding the first applicant

**STEP 1: COMPREHENSIVE APPLICANT EVIDENCE SCAN**

### Primary Applicant Sections (MANDATORY SEARCH)
**Search these sections systematically:**
- "Applicant Information" sections
- "Assignee Information" blocks
- "Company Information" areas
- "Organization" sections
- "Entity Information" blocks

**Evidence to Extract for EACH applicant found:**
- Raw Text Block: "[Quote complete section containing applicant info]"
- Organization Name: "[Exact company/organization name]"
- Individual Name: "[If person: given name, family name]"
- Complete Address: "[Full business/mailing address]"
- Source Location: Page X, Section Y
- Confidence: High/Medium/Low

### Secondary Applicant Sections (THOROUGH SEARCH)
**Check these additional locations:**
- Document headers with company letterhead
- Footer information with entity details
- "Correspondence Address" sections
- "Attorney Information" with client details
- "Customer Number" references
- Assignment and transfer statements

**Evidence Pattern:**
- Section Type: "[Header/Footer/Correspondence/etc.]"
- Entity Found: "[Any company or organization name]"
- Address Info: "[Any address information]"
- Contact Details: "[Customer numbers, emails, phones]"
- Source Location: Page X, Section Y

### Contextual Applicant Clues (DETECTIVE SEARCH)
**Look for these indicators:**
- Legal entity suffixes: "Inc.", "LLC", "Corp.", "Ltd.", "Co."
- Business address patterns (not residential)
- Customer number patterns (5-6 digit numbers)
- Corporate email domains
- Multiple address blocks (inventor vs. company addresses)

**Evidence Pattern:**
- Clue Type: "[Legal suffix/Business address/Customer number/etc.]"
- Text Found: "[Exact text containing the clue]"
- Potential Applicant: "[Inferred company/entity name]"
- Source Location: Page X, Section Y

### Multi-Applicant Detection Strategy
**CRITICAL INSTRUCTIONS:**
1. **Count as you go**: Keep track of how many distinct applicants you find
2. **Don't stop at one**: If you find one applicant, actively search for more
3. **Check for patterns**: Look for "Applicant 1:", "Applicant 2:", numbering
4. **Cross-reference**: Compare addresses to distinguish inventors from applicants
5. **Be exhaustive**: Better to over-extract than miss applicants

### Invention Title Search
Look for "Title of Invention", "Title:", or similar headers.

### Inventor Information Gathering
Look for "Inventor Information", "Legal Name", "Given Name", "Family Name" sections.
Extract EVERY inventor found, even if information is incomplete.
**CRITICAL**: Extract complete postal codes (02115, 02138, 94103, 94301) from inventor addresses.

### Entity Status Information Gathering
**CRITICAL ENTITY STATUS EXTRACTION:**
- Look for entity status indicators: "Small Entity", "Micro Entity", "Large Entity"
- Check for checkboxes (â˜ â˜‘ âœ“), selected options, or written text
- Search in headers, form fields, declaration sections, and fee information
- Look for phrases like "Entity Status:", "Fee Category:", "Small Entity Status"
- If you see any fee amounts or entity-related information, extract it
- Default to "Small Entity" if any small entity indicators are found

### Correspondence Information Gathering
Look for "Customer Number", "Correspondence Address", email addresses.

### Priority Claims Information Gathering
Look for "claims benefit of", "continuation of", "provisional application".

**MULTI-APPLICANT EVIDENCE SUMMARY:**
```
APPLICANT SEARCH RESULTS:
Total Applicants Found: [NUMBER]

APPLICANT #1:
- Organization Name: "[Company name or null]"
- Individual Name: "[Person name or null]"
- Address: "[Complete address]"
- Source: Page X, Section Y
- Confidence: High/Medium/Low

APPLICANT #2:
- Organization Name: "[Company name or null]"
- Individual Name: "[Person name or null]"
- Address: "[Complete address]"
- Source: Page X, Section Y
- Confidence: High/Medium/Low

[Continue for all applicants found...]

SEARCH COMPLETENESS CHECK:
- Primary sections searched: âœ“/âœ—
- Secondary sections searched: âœ“/âœ—
- Contextual clues analyzed: âœ“/âœ—
- Cross-section validation performed: âœ“/âœ—
```

**OUTPUT FORMAT:**
Provide a structured response with evidence for each category found, with special emphasis on complete applicant information.
"""
    
    def _get_xfa_specific_instructions(self) -> str:
        """
        XFA form specific instructions
        """
        return """

**XFA FORM SPECIFIC INSTRUCTIONS:**
You are analyzing an XFA (Dynamic) form with structured XML data.
1. Look for XML tags like `<GivenName>`, `<FamilyName>`, `<MailingAddress>`
2. Search for inventor sequences: `<Inventor_1>`, `<Inventor_2>`, etc.
3. Find applicant/assignee information in dedicated XML sections
4. Extract form field values, not just field names
"""
    
    def _get_form_fields_instructions(self) -> str:
        """
        Form fields specific instructions
        """
        return """

**FORM FIELDS SPECIFIC INSTRUCTIONS:**
You are analyzing a document with structured form fields.
1. Look for "--- FORM FIELD DATA ---" sections
2. Extract key-value pairs from form fields
3. Map field names to appropriate data categories
4. Handle repeating field patterns for multiple inventors
"""
    
    def _get_vision_analysis_instructions(self) -> str:
        """
        Vision analysis specific instructions
        """
        return """

**VISION ANALYSIS SPECIFIC INSTRUCTIONS:**
You are analyzing a document visually (may be scanned or image-based).
1. Use visual structure to understand document layout
2. Identify table structures and form layouts
3. Read text carefully, noting any OCR quality issues
4. Look for visual indicators like checkboxes, form fields
"""
    
    def _get_text_extraction_instructions(self) -> str:
        """
        Text extraction specific instructions
        """
        return """

**TEXT EXTRACTION SPECIFIC INSTRUCTIONS:**
You are analyzing extracted text content from a document.
1. Look for section headers and structured text
2. Identify repeating patterns for multiple inventors
3. Parse addresses and contact information carefully
4. Note any formatting that indicates structure
"""


class JSONGenerationPrompts:
    """
    Prompts for JSON generation phase
    """
    
    def create_json_generation_prompt(self, document_evidence: DocumentEvidence) -> str:
        """
        Create JSON generation prompt based on gathered evidence
        """
        evidence_summary = self._summarize_evidence(document_evidence)
        
        return f"""
Based ONLY on the evidence gathered below, generate a single, valid JSON object with ALL applicants found.

**CRITICAL MULTI-APPLICANT RULES:**
- EXTRACT ALL APPLICANTS found in evidence - do NOT limit to just one
- If you find multiple companies/organizations, include ALL of them in the applicants array
- Each applicant must be a separate object in the applicants array
- Ensure complete information for each applicant when available
- Use null for missing fields, never omit applicants due to incomplete data

**CORE RULES:**
- Use `null` if a specific field is absolutely not found in the evidence
- Format all dates as "YYYY-MM-DD" (convert from any format found)
- Ensure "country" is a 2-letter ISO code (e.g., "US") or full country name
- Do NOT make up, assume, or infer any data not explicitly found in evidence
- If evidence is unclear or ambiguous, use `null` rather than guess

**MULTI-APPLICANT VALIDATION CHECKLIST:**
Before generating JSON, verify:
- âœ… Did you include ALL applicants found in evidence (not just the first one)?
- âœ… Did you check all evidence sections for additional applicants?
- âœ… Did you distinguish between inventors and applicants properly?
- âœ… Did you include both individual and organizational applicants?
- âœ… Did you preserve complete address information for each applicant?

**POSTAL CODE CRITICAL INSTRUCTION:**
- EXTRACT COMPLETE postal codes from inventor addresses
- Look for patterns like: 02115, 02138, 94103, 94301, W1G 9PR
- Parse addresses carefully to separate postal codes from other address components

**EVIDENCE SUMMARY:**
{evidence_summary}

**STEP 2: JSON GENERATION WITH MULTI-APPLICANT FOCUS**

Generate a JSON object with this structure:

{{
  "title": "String from evidence or null",
  "application_number": "String from evidence or null",
  "filing_date": "YYYY-MM-DD format or null",
  "entity_status": "String from evidence or null (Small Entity, Micro Entity, Large Entity)",
  "attorney_docket_number": "String from evidence or null",
  "total_drawing_sheets": "Integer from evidence or null",
  "customer_number": "String from evidence or null",
  "correspondence_email": "String from evidence or null",
  "inventors": [
    {{
      "given_name": "String from evidence",
      "middle_name": "COMPLETE middle name from evidence - DO NOT TRUNCATE (e.g. Michael, Elizabeth)",
      "family_name": "String from evidence",
      "full_name": "String from evidence or null",
      "street_address": "String from evidence",
      "city": "String from evidence",
      "state": "String from evidence",
      "postal_code": "REQUIRED - Extract complete postal code (e.g. 02115, 02138, 94103, 94301)",
      "country": "String from evidence",
      "citizenship": "REQUIRED - String from evidence (e.g. United States, US)",
      "completeness": "complete|partial_name|partial_address|incomplete",
      "confidence_score": "0.0-1.0 (float)"
    }}
  ],
  "applicants": [
    {{
      "applicant_sequence": 1,
      "is_assignee": true/false,
      "organization_name": "String from evidence or null",
      "individual_given_name": "String from evidence or null",
      "individual_family_name": "String from evidence or null",
      "street_address": "String from evidence",
      "city": "String from evidence",
      "state": "String from evidence",
      "postal_code": "String from evidence",
      "country": "String from evidence",
      "customer_number": "String from evidence or null",
      "email_address": "String from evidence or null",
      "relationship_to_inventors": "same_as_inventor|separate_entity|unclear",
      "legal_entity_type": "corporation|llc|individual|partnership|other|null",
      "completeness": "complete|partial_name|partial_address|incomplete",
      "confidence_score": "0.0-1.0 (float)",
      "evidence_sources": ["primary_section|secondary_section|contextual"]
    }},
    {{
      "applicant_sequence": 2,
      "is_assignee": true/false,
      "organization_name": "String from evidence or null",
      "individual_given_name": "String from evidence or null",
      "individual_family_name": "String from evidence or null",
      "street_address": "String from evidence",
      "city": "String from evidence",
      "state": "String from evidence",
      "postal_code": "String from evidence",
      "country": "String from evidence",
      "customer_number": "String from evidence or null",
      "email_address": "String from evidence or null",
      "relationship_to_inventors": "same_as_inventor|separate_entity|unclear",
      "legal_entity_type": "corporation|llc|individual|partnership|other|null",
      "completeness": "complete|partial_name|partial_address|incomplete",
      "confidence_score": "0.0-1.0 (float)",
      "evidence_sources": ["primary_section|secondary_section|contextual"]
    }}
  ]
}}

**FINAL MULTI-APPLICANT VALIDATION:**
After generating JSON, confirm:
- âœ… Total applicants in JSON matches total found in evidence
- âœ… Each applicant has unique information (no obvious duplicates)
- âœ… All applicants have at least organization_name OR individual names
- âœ… Applicant addresses are distinct from inventor addresses
- âœ… Confidence scores reflect evidence quality
- âœ… No applicants were omitted due to incomplete information

**QUALITY ENHANCEMENT RULES:**
If any validation fails:
1. Return to evidence and re-extract missing applicants
2. Consolidate similar entities using deduplication rules
3. Clarify relationships using relationship analysis
4. Enhance incomplete data using cross-referencing
"""
    
    def _summarize_evidence(self, document_evidence: DocumentEvidence) -> str:
        """
        Summarize gathered evidence for JSON generation prompt
        """
        summary_parts = []
        
        # DIAGNOSTIC LOGGING: Track evidence completeness
        logger.info(f"EVIDENCE SUMMARY - Title: {'FOUND' if document_evidence.title_evidence else 'MISSING'}")
        logger.info(f"EVIDENCE SUMMARY - App Number: {'FOUND' if document_evidence.application_number_evidence else 'MISSING'}")
        logger.info(f"EVIDENCE SUMMARY - Entity Status: {'FOUND' if document_evidence.entity_status_evidence else 'MISSING'}")
        logger.info(f"EVIDENCE SUMMARY - Inventors Found: {len(document_evidence.inventor_evidence)}")
        logger.info(f"EVIDENCE SUMMARY - Applicants Found: {len(document_evidence.applicant_evidence)}")
        
        # Title evidence
        if document_evidence.title_evidence:
            summary_parts.append(f"TITLE: {document_evidence.title_evidence.raw_text}")
        
        # Application number evidence
        if document_evidence.application_number_evidence:
            summary_parts.append(f"APPLICATION NUMBER: {document_evidence.application_number_evidence.raw_text}")
        
        # Entity status evidence - ADD DIAGNOSTIC
        if document_evidence.entity_status_evidence:
            summary_parts.append(f"ENTITY STATUS: {document_evidence.entity_status_evidence.raw_text}")
            logger.info(f"ENTITY STATUS FOUND: {document_evidence.entity_status_evidence.raw_text}")
        else:
            logger.warning("ENTITY STATUS EVIDENCE MISSING")
        
        # Inventor evidence - ADD POSTAL CODE DIAGNOSTIC
        for i, inv_evidence in enumerate(document_evidence.inventor_evidence):
            inv_summary = f"INVENTOR {i+1}:"
            if inv_evidence.given_name_evidence:
                inv_summary += f" Given: {inv_evidence.given_name_evidence.raw_text}"
            if inv_evidence.family_name_evidence:
                inv_summary += f" Family: {inv_evidence.family_name_evidence.raw_text}"
            
            # DIAGNOSTIC: Check for postal codes in address evidence
            postal_code_found = False
            for addr in inv_evidence.address_evidence:
                inv_summary += f" Address: {addr.raw_text}"
                # Check if this address contains postal code patterns
                if any(pattern in addr.raw_text for pattern in ['02115', '02138', '94103', '94301', r'\d{5}']):
                    postal_code_found = True
            
            if not postal_code_found:
                logger.warning(f"INVENTOR {i+1} MISSING POSTAL CODE in address evidence")
            
            summary_parts.append(inv_summary)
        
        # Applicant evidence - ENHANCED MULTI-APPLICANT DIAGNOSTIC
        total_applicants = len(document_evidence.applicant_evidence)
        logger.info(f"EVIDENCE SUMMARY - Total Applicants Found: {total_applicants}")
        
        for i, app_evidence in enumerate(document_evidence.applicant_evidence):
            app_summary = f"APPLICANT {i+1}:"
            
            # Organization name
            if app_evidence.organization_name_evidence:
                org_name = app_evidence.organization_name_evidence.raw_text
                app_summary += f" Org: {org_name}"
                logger.info(f"APPLICANT {i+1} ORG: {org_name}")
                
                # Check source section for diagnostic
                source_section = app_evidence.organization_name_evidence.source_location.section
                logger.info(f"APPLICANT {i+1} SOURCE: {source_section}")
            
            # Individual name (if applicable)
            if app_evidence.individual_name_evidence:
                for name_evidence in app_evidence.individual_name_evidence:
                    app_summary += f" Individual: {name_evidence.raw_text}"
            
            # Address information
            address_count = len(app_evidence.address_evidence)
            if address_count > 0:
                app_summary += f" Addresses: {address_count}"
                for addr in app_evidence.address_evidence:
                    logger.info(f"APPLICANT {i+1} ADDRESS: {addr.raw_text}")
            
            # Contact information
            contact_count = len(app_evidence.contact_evidence)
            if contact_count > 0:
                app_summary += f" Contacts: {contact_count}"
            
            # Completeness and confidence
            app_summary += f" Completeness: {app_evidence.completeness.value}"
            app_summary += f" Confidence: {app_evidence.overall_confidence.value}"
            
            summary_parts.append(app_summary)
        
        # ENHANCED DIAGNOSTIC: Multi-applicant detection analysis
        if total_applicants == 0:
            logger.error("CRITICAL: No applicants found! This should not happen.")
            summary_parts.append("âš ï¸ NO APPLICANTS DETECTED - REVIEW EVIDENCE GATHERING")
        elif total_applicants == 1:
            logger.warning("SINGLE APPLICANT: Only found 1 applicant. Check for additional applicants in:")
            logger.warning("  - Secondary sections (headers, correspondence)")
            logger.warning("  - Multiple company mentions")
            logger.warning("  - Assignment statements")
            summary_parts.append("âš ï¸ SINGLE APPLICANT DETECTED - VERIFY NO ADDITIONAL APPLICANTS EXIST")
        else:
            logger.info(f"MULTI-APPLICANT SUCCESS: Found {total_applicants} applicants")
            summary_parts.append(f"âœ… MULTI-APPLICANT DETECTION: {total_applicants} applicants found")
        
        # Check for specific test case patterns
        applicant_names = []
        for app_evidence in document_evidence.applicant_evidence:
            if app_evidence.organization_name_evidence:
                applicant_names.append(app_evidence.organization_name_evidence.raw_text)
        
        # Log applicant names for diagnostic purposes
        if applicant_names:
            logger.info(f"APPLICANT NAMES DETECTED: {', '.join(applicant_names)}")
            summary_parts.append(f"APPLICANT NAMES: {', '.join(applicant_names)}")
        
        return "\n".join(summary_parts)